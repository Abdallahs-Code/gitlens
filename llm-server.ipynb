{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31286,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pyarrow\nimport os\nos.system(f\"pip install unsloth fastapi uvicorn pyngrok peft transformers==4.56.2 pyarrow=={pyarrow.__version__}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from unsloth import FastLanguageModel\nimport torch\n\nmax_seq_length = 2048\ndtype = None\nload_in_4bit = True\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/Llama-3.1-8B\",\n    max_seq_length = max_seq_length,\n    dtype = dtype,\n    load_in_4bit = load_in_4bit,\n)\n\nfrom peft import PeftModel\nmodel = PeftModel.from_pretrained(model, \"hg_username/your_model_name\")\nFastLanguageModel.for_inference(model)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import asyncio\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\n\napp = FastAPI()\n\nlock = asyncio.Lock()\nwaiting_count = 0\nMAX = 10\n\nalpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\n{}\n\n### Input:\n{}\n\n### Response:\n{}\"\"\"\n\nclass RequestBody(BaseModel):\n    data: str\n\n@app.post(\"/evaluate\")\nasync def evaluate(body: RequestBody):\n    global waiting_count\n\n    if waiting_count >= MAX:\n        return {\"status\": \"unavailable\", \"message\": \"Server is busy, please try again later.\"}\n\n    waiting_count += 1\n    try:\n        async with lock:\n            prompt = alpaca_prompt.format(\n                \"Evaluate the candidate's fit for the job and respond with a JSON object containing verdict, strengths, gaps, and explanation.\",\n                f\"{body.data}\",\n                \"\"\n            )\n            inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n            outputs = model.generate(**inputs, max_new_tokens=512, use_cache=True)\n            decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n            response_text = decoded.split(\"### Response:\")[-1].strip()\n            return {\"status\": \"ok\", \"result\": response_text}\n    finally:\n        waiting_count -= 1","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import nest_asyncio\nimport uvicorn\nfrom pyngrok import ngrok\n\nngrok.set_auth_token(\"ngrok_token\")\nnest_asyncio.apply()\n\npublic_url = ngrok.connect(8000)\nprint(f\"Public URL: {public_url}\")\n\nconfig = uvicorn.Config(app, host=\"0.0.0.0\", port=8000)\nserver = uvicorn.Server(config)\nawait server.serve()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}